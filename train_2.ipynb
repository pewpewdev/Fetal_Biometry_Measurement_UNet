{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oEzVXtH5v-W",
        "outputId": "a02ef95b-a8c4-4e28-905b-8e2f69062238"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        },
        "id": "7rymPvYq5nW9",
        "outputId": "26758ec7-d379-459a-d73b-2ddec9d79118"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 1/5 [Train]:   0%|          | 0/142 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([4, 8])) that is different to the input size (torch.Size([4, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "Epoch 1/5 [Train]:   0%|          | 0/142 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (4) must match the size of tensor b (8) at non-singleton dimension 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-d8f2b46fb3fe>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3789\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3791\u001b[0;31m     \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3792\u001b[0m     return torch._C._nn.mse_loss(\n\u001b[1;32m   3793\u001b[0m         \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (8) at non-singleton dimension 1"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Configuration\n",
        "DATA_PATH = \"/content/drive/MyDrive/filtered_image\"\n",
        "CSV_PATH = \"/content/drive/MyDrive/landmark_detection/train.csv\"\n",
        "IMAGE_SIZE = (800, 540)  # U-Net typically works with 256x256 or 512x512 images\n",
        "BATCH_SIZE = 4\n",
        "NUM_EPOCHS = 5\n",
        "NUM_LANDMARKS = 8  # 4 points Ã— (x, y)\n",
        "THRESHOLD = 5.0  # Pixel threshold for accuracy computation\n",
        "DEVICE = torch.device(\"cuda:0\" )\n",
        "# Custom Dataset\n",
        "class FetalLandmarkDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.dataframe = dataframe\n",
        "        self.scaler = StandardScaler()\n",
        "        self.coordinates = self.scaler.fit_transform(dataframe.iloc[:, 1:].values)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.dataframe.iloc[idx, 0]\n",
        "        img_path = f\"{DATA_PATH}/{img_name}\"\n",
        "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "        #image = cv2.resize(image, IMAGE_SIZE)\n",
        "        image = image.astype(np.float32) / 255.0\n",
        "\n",
        "        # Convert grayscale to 3-channel by repeating the grayscale channel\n",
        "        image = np.stack([image] * 3, axis=-1)  # Shape: (H, W, 3)\n",
        "        image = np.transpose(image, (2, 0, 1))  # Shape: (3, H, W)\n",
        "\n",
        "        coords = self.coordinates[idx]\n",
        "\n",
        "        return torch.tensor(image, dtype=torch.float32), torch.tensor(coords, dtype=torch.float32)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=NUM_LANDMARKS):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        def conv_block(in_channels, out_channels):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "                nn.ReLU(inplace=True)\n",
        "            )\n",
        "\n",
        "        def up_conv(in_channels, out_channels):\n",
        "            return nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder1 = conv_block(in_channels, 64)\n",
        "        self.encoder2 = conv_block(64, 128)\n",
        "        self.encoder3 = conv_block(128, 256)\n",
        "        self.encoder4 = conv_block(256, 512)\n",
        "\n",
        "        # Pooling with ceil_mode to handle odd dimensions\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = conv_block(512, 1024)\n",
        "\n",
        "        # Decoder\n",
        "        self.upconv4 = up_conv(1024, 512)\n",
        "        self.decoder4 = conv_block(1024, 512)\n",
        "        self.upconv3 = up_conv(512, 256)\n",
        "        self.decoder3 = conv_block(512, 256)\n",
        "        self.upconv2 = up_conv(256, 128)\n",
        "        self.decoder2 = conv_block(256, 128)\n",
        "        self.upconv1 = up_conv(128, 64)\n",
        "        self.decoder1 = conv_block(128, 64)\n",
        "\n",
        "        # Final regression head\n",
        "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        e1 = self.encoder1(x)          # (64, H, W)\n",
        "        e2 = self.encoder2(self.pool(e1))  # (128, H/2, W/2)\n",
        "        e3 = self.encoder3(self.pool(e2))  # (256, H/4, W/4)\n",
        "        e4 = self.encoder4(self.pool(e3))  # (512, H/8, W/8)\n",
        "\n",
        "        # Bottleneck\n",
        "        bottleneck = self.bottleneck(self.pool(e4))  # (1024, H/16, W/16)\n",
        "\n",
        "        # Decoder with cropping\n",
        "        d4 = self.upconv4(bottleneck)       # (512, H/8, W/8)\n",
        "        d4 = self.crop(d4, e4)              # Ensure matching dimensions\n",
        "        d4 = torch.cat([d4, e4], dim=1)     # (1024, H/8, W/8)\n",
        "        d4 = self.decoder4(d4)              # (512, H/8, W/8)\n",
        "\n",
        "        d3 = self.upconv3(d4)               # (256, H/4, W/4)\n",
        "        d3 = self.crop(d3, e3)\n",
        "        d3 = torch.cat([d3, e3], dim=1)     # (512, H/4, W/4)\n",
        "        d3 = self.decoder3(d3)              # (256, H/4, W/4)\n",
        "\n",
        "        d2 = self.upconv2(d3)               # (128, H/2, W/2)\n",
        "        d2 = self.crop(d2, e2)\n",
        "        d2 = torch.cat([d2, e2], dim=1)     # (256, H/2, W/2)\n",
        "        d2 = self.decoder2(d2)              # (128, H/2, W/2)\n",
        "\n",
        "        d1 = self.upconv1(d2)               # (64, H, W)\n",
        "        d1 = self.crop(d1, e1)\n",
        "        d1 = torch.cat([d1, e1], dim=1)     # (128, H, W)\n",
        "        d1 = self.decoder1(d1)              # (64, H, W)\n",
        "\n",
        "        # Final output\n",
        "        output = self.final_conv(d1)        # (out_channels, H, W)\n",
        "        output = self.global_pool(output)   # (out_channels, 1, 1)\n",
        "        output = output.view(output.size(0), -1)\n",
        "        return output\n",
        "\n",
        "    def crop(self, source, target):\n",
        "        # Crop source to match target dimensions\n",
        "        _, _, h, w = target.size()\n",
        "        return source[:, :, :h, :w]\n",
        "\n",
        "# Load and prepare data\n",
        "full_df = pd.read_csv(CSV_PATH)\n",
        "train_df, val_df = train_test_split(full_df, test_size=0.2, random_state=42)\n",
        "train_dataset = FetalLandmarkDataset(train_df)\n",
        "val_dataset = FetalLandmarkDataset(val_df)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=4)\n",
        "\n",
        "# Initialize model\n",
        "model = UNet().to(DEVICE)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.01)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Training loop\n",
        "best_val_loss = float('inf')\n",
        "train_losses, val_losses = [], []\n",
        "train_accuracies, val_accuracies = [], []\n",
        "\n",
        "def compute_accuracy(preds, targets, threshold=THRESHOLD):\n",
        "    preds = preds.cpu().detach().numpy()\n",
        "    targets = targets.cpu().detach().numpy()\n",
        "    errors = np.linalg.norm(preds - targets, axis=1)\n",
        "    return np.mean(errors < threshold) * 100\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    running_loss, correct_train, total_train = 0.0, 0, 0\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Train]\")\n",
        "\n",
        "    for images, targets in progress_bar:\n",
        "        images, targets = images.to(DEVICE), targets.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        correct_train += compute_accuracy(outputs, targets)\n",
        "        total_train += 1\n",
        "        progress_bar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "    train_loss = running_loss / len(train_loader.dataset)\n",
        "    train_acc = correct_train / total_train\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss, correct_val, total_val = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, targets in val_loader:\n",
        "            images, targets = images.to(DEVICE), targets.to(DEVICE)\n",
        "            outputs = model(images)\n",
        "            val_loss += criterion(outputs, targets).item() * images.size(0)\n",
        "            correct_val += compute_accuracy(outputs, targets)\n",
        "            total_val += 1\n",
        "\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    val_acc = correct_val / total_val\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_acc)\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    print(f\"\\nEpoch {epoch+1} Summary:\")\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), \"best_unet_landmark_model.pth\")\n",
        "        print(\"Saved new best model!\")\n",
        "\n",
        "print(\"Training completed!\")\n",
        "# Print final training statistics\n",
        "print(\"\\nFinal Training Statistics:\")\n",
        "print(f\"Total Training Accuracy: {train_accuracies[-1]:.2f}%\")\n",
        "print(f\"Total Training Loss: {train_losses[-1]:.4f}\")\n",
        "print(f\"Total Validation Accuracy: {val_accuracies[-1]:.2f}%\")\n",
        "print(f\"Total Validation Loss: {val_losses[-1]:.4f}\")\n",
        "\n",
        "\n",
        "# Plot training curves\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.title(\"Loss Progress\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accuracies, label='Training Accuracy')\n",
        "plt.plot(val_accuracies, label='Validation Accuracy')\n",
        "plt.title(\"Accuracy Progress\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), f'/content/drive/MyDrive/landmark_detection/unet_epoch_iteration_1.pth')"
      ],
      "metadata": {
        "id": "zGOVzD1OAGQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYHRGPPn5nXF",
        "outputId": "63925c9c-b4c9-443b-fd38-eedcfc94fe16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions saved to predictions_unet.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define constants\n",
        "IMAGE_SIZE = (224, 224)  # Ensure this matches the training image size\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def predict_and_save_to_csv(image_folder, output_csv, model):\n",
        "    model.to(DEVICE)\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    # Initialize a list to store predictions\n",
        "    predictions = []\n",
        "\n",
        "    # Iterate over all images in the folder\n",
        "    for img_name in os.listdir(image_folder):\n",
        "        img_path = os.path.join(image_folder, img_name)\n",
        "\n",
        "        # Load image in grayscale\n",
        "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "        if image is None:\n",
        "            print(f\"Warning: Failed to read image {img_name}\")\n",
        "            continue\n",
        "\n",
        "        # Convert grayscale to 3-channel RGB\n",
        "        image_rgb = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "        # Resize image to match the input size expected by the model\n",
        "        image_resized = cv2.resize(image_rgb, IMAGE_SIZE)\n",
        "        image_resized = image_resized.astype(np.float32) / 255.0  # Normalize\n",
        "\n",
        "        # Convert to torch tensor and add batch dimension\n",
        "        image_tensor = torch.from_numpy(image_resized).permute(2, 0, 1).unsqueeze(0).to(DEVICE)  # Shape: (1, 3, 224, 224)\n",
        "\n",
        "        # Get the predictions from the model\n",
        "        with torch.no_grad():\n",
        "            landmarks = model(image_tensor)  # Output shape: (1, 8)\n",
        "\n",
        "        # Convert predictions to numpy and flatten\n",
        "        landmarks_np = landmarks.cpu().numpy().flatten()\n",
        "\n",
        "        # Append predictions with image name\n",
        "        predictions.append([img_name] + landmarks_np.tolist())\n",
        "\n",
        "    # Create a dataframe and save to CSV\n",
        "    df_predictions = pd.DataFrame(predictions, columns=[\n",
        "        \"image_name\", \"ofd_1_x\", \"ofd_1_y\", \"ofd_2_x\", \"ofd_2_y\",\n",
        "        \"bpd_1_x\", \"bpd_1_y\", \"bpd_2_x\", \"bpd_2_y\"\n",
        "    ])\n",
        "    df_predictions.to_csv(output_csv, index=False)\n",
        "    print(f\"Predictions saved to {output_csv}\")\n",
        "\n",
        "# Test folder path and output CSV path\n",
        "image_folder = \"/content/drive/MyDrive/landmark_detection/test\"  # Update with your test folder\n",
        "output_csv = \"predictions_unet.csv\"  # Update output file name\n",
        "\n",
        "# Ensure the model is loaded before calling the function\n",
        "predict_and_save_to_csv(image_folder, output_csv, model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKn7PP6I5nXG",
        "outputId": "9ed38f5f-fe19-4227-906a-c7b066eef1d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Absolute Errors (in pixels):\n",
            "Landmark 1: nan pixels\n",
            "Landmark 2: nan pixels\n",
            "Landmark 3: nan pixels\n",
            "Landmark 4: nan pixels\n",
            "\n",
            "Relative Errors (% of image size):\n",
            "Landmark 1: nan%\n",
            "Landmark 2: nan%\n",
            "Landmark 3: nan%\n",
            "Landmark 4: nan%\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def compare_predictions(pred_csv, ground_truth_csv, image_size=299):\n",
        "    # Load predicted CSV\n",
        "    df_pred = pd.read_csv(pred_csv)\n",
        "\n",
        "    # Load ground truth CSV\n",
        "    df_gt = pd.read_csv(ground_truth_csv)\n",
        "\n",
        "    # Merge both dataframes on the image_name column\n",
        "    df_merged = pd.merge(df_pred, df_gt, on=\"image_name\", suffixes=('_pred', '_gt'))\n",
        "\n",
        "    # Compute the errors for each landmark (for each coordinate pair: x and y)\n",
        "    errors = []\n",
        "    relative_errors = []\n",
        "\n",
        "    for i in range(1, 5):  # Assuming 4 landmarks: ofd_1, ofd_2, bpd_1, bpd_2\n",
        "        x_pred_col = f\"ofd_{i}_x_pred\" if i <= 2 else f\"bpd_{i-2}_x_pred\"\n",
        "        y_pred_col = f\"ofd_{i}_y_pred\" if i <= 2 else f\"bpd_{i-2}_y_pred\"\n",
        "\n",
        "        x_gt_col = f\"ofd_{i}_x_gt\" if i <= 2 else f\"bpd_{i-2}_x_gt\"\n",
        "        y_gt_col = f\"ofd_{i}_y_gt\" if i <= 2 else f\"bpd_{i-2}_y_gt\"\n",
        "\n",
        "        # Check if the columns exist in the dataframe\n",
        "        if all(col in df_merged.columns for col in [x_pred_col, y_pred_col, x_gt_col, y_gt_col]):\n",
        "            # Compute absolute errors\n",
        "            x_error = abs(df_merged[x_pred_col] - df_merged[x_gt_col])\n",
        "            y_error = abs(df_merged[y_pred_col] - df_merged[y_gt_col])\n",
        "\n",
        "            # Compute mean absolute error (AAE)\n",
        "            mean_x_error = x_error.mean()\n",
        "            mean_y_error = y_error.mean()\n",
        "            mean_error = (mean_x_error + mean_y_error) / 2  # Average of x and y errors\n",
        "\n",
        "            errors.append(mean_error)\n",
        "\n",
        "            # Compute relative error in percentage\n",
        "            relative_error = (mean_error / image_size) * 100\n",
        "            relative_errors.append(relative_error)\n",
        "        else:\n",
        "            print(f\"Warning: One or more columns for landmark {i} are missing.\")\n",
        "\n",
        "    # Print average absolute errors\n",
        "    print(\"Average Absolute Errors (in pixels):\")\n",
        "    for i, error in enumerate(errors):\n",
        "        print(f\"Landmark {i+1}: {error:.4f} pixels\")\n",
        "\n",
        "    # Print relative errors\n",
        "    print(\"\\nRelative Errors (% of image size):\")\n",
        "    for i, rel_error in enumerate(relative_errors):\n",
        "        print(f\"Landmark {i+1}: {rel_error:.2f}%\")\n",
        "\n",
        "# Paths to the predictions CSV and ground truth CSV\n",
        "pred_csv = \"predictions_unet.csv\"\n",
        "ground_truth_csv = \"/content/drive/MyDrive/landmark_detection/test.csv\"\n",
        "\n",
        "# Compare predictions with ground truth\n",
        "compare_predictions(pred_csv, ground_truth_csv)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7O63Unc5nXG",
        "outputId": "8a5cff47-0a8a-4497-c10e-58d7e91e73f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictions saved to predictions_unet_train.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define constants\n",
        "IMAGE_SIZE = (224, 224)  # Ensure this matches the training image size\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def predict_and_save_to_csv(image_folder, output_csv, model):\n",
        "    model.to(DEVICE)\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    # Initialize a list to store predictions\n",
        "    predictions = []\n",
        "\n",
        "    # Iterate over all images in the folder\n",
        "    for img_name in os.listdir(image_folder):\n",
        "        img_path = os.path.join(image_folder, img_name)\n",
        "\n",
        "        # Load image in grayscale\n",
        "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "        if image is None:\n",
        "            print(f\"Warning: Failed to read image {img_name}\")\n",
        "            continue\n",
        "\n",
        "        # Convert grayscale to 3-channel RGB\n",
        "        image_rgb = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "        # Resize image to match the input size expected by the model\n",
        "        image_resized = cv2.resize(image_rgb, IMAGE_SIZE)\n",
        "        image_resized = image_resized.astype(np.float32) / 255.0  # Normalize\n",
        "\n",
        "        # Convert to torch tensor and add batch dimension\n",
        "        image_tensor = torch.from_numpy(image_resized).permute(2, 0, 1).unsqueeze(0).to(DEVICE)  # Shape: (1, 3, 224, 224)\n",
        "\n",
        "        # Get the predictions from the model\n",
        "        with torch.no_grad():\n",
        "            landmarks = model(image_tensor)  # Output shape: (1, 8)\n",
        "\n",
        "        # Convert predictions to numpy and flatten\n",
        "        landmarks_np = landmarks.cpu().numpy().flatten()\n",
        "\n",
        "        # Append predictions with image name\n",
        "        predictions.append([img_name] + landmarks_np.tolist())\n",
        "\n",
        "    # Create a dataframe and save to CSV\n",
        "    df_predictions = pd.DataFrame(predictions, columns=[\n",
        "        \"image_name\", \"ofd_1_x\", \"ofd_1_y\", \"ofd_2_x\", \"ofd_2_y\",\n",
        "        \"bpd_1_x\", \"bpd_1_y\", \"bpd_2_x\", \"bpd_2_y\"\n",
        "    ])\n",
        "    df_predictions.to_csv(output_csv, index=False)\n",
        "    print(f\"Predictions saved to {output_csv}\")\n",
        "\n",
        "# Test folder path and output CSV path\n",
        "image_folder = \"image_train\"  # Update with your test folder\n",
        "output_csv = \"predictions_unet_train.csv\"  # Update output file name\n",
        "\n",
        "# Ensure the model is loaded before calling the function\n",
        "predict_and_save_to_csv(image_folder, output_csv, model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rJJ6Cpv5nXH",
        "outputId": "5142c9fb-a640-4635-dc86-ae62260b068b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Absolute Errors (in pixels):\n",
            "Landmark 1: 265.7234 pixels\n",
            "Landmark 2: 408.0808 pixels\n",
            "Landmark 3: 239.0683 pixels\n",
            "Landmark 4: 428.7966 pixels\n",
            "\n",
            "Relative Errors (% of image size):\n",
            "Landmark 1: 88.87%\n",
            "Landmark 2: 136.48%\n",
            "Landmark 3: 79.96%\n",
            "Landmark 4: 143.41%\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def compare_predictions(pred_csv, ground_truth_csv, image_size=299):\n",
        "    # Load predicted CSV\n",
        "    df_pred = pd.read_csv(pred_csv)\n",
        "\n",
        "    # Load ground truth CSV\n",
        "    df_gt = pd.read_csv(ground_truth_csv)\n",
        "\n",
        "    # Merge both dataframes on the image_name column\n",
        "    df_merged = pd.merge(df_pred, df_gt, on=\"image_name\", suffixes=('_pred', '_gt'))\n",
        "\n",
        "    # Compute the errors for each landmark (for each coordinate pair: x and y)\n",
        "    errors = []\n",
        "    relative_errors = []\n",
        "\n",
        "    for i in range(1, 5):  # Assuming 4 landmarks: ofd_1, ofd_2, bpd_1, bpd_2\n",
        "        x_pred_col = f\"ofd_{i}_x_pred\" if i <= 2 else f\"bpd_{i-2}_x_pred\"\n",
        "        y_pred_col = f\"ofd_{i}_y_pred\" if i <= 2 else f\"bpd_{i-2}_y_pred\"\n",
        "\n",
        "        x_gt_col = f\"ofd_{i}_x_gt\" if i <= 2 else f\"bpd_{i-2}_x_gt\"\n",
        "        y_gt_col = f\"ofd_{i}_y_gt\" if i <= 2 else f\"bpd_{i-2}_y_gt\"\n",
        "\n",
        "        # Check if the columns exist in the dataframe\n",
        "        if all(col in df_merged.columns for col in [x_pred_col, y_pred_col, x_gt_col, y_gt_col]):\n",
        "            # Compute absolute errors\n",
        "            x_error = abs(df_merged[x_pred_col] - df_merged[x_gt_col])\n",
        "            y_error = abs(df_merged[y_pred_col] - df_merged[y_gt_col])\n",
        "\n",
        "            # Compute mean absolute error (AAE)\n",
        "            mean_x_error = x_error.mean()\n",
        "            mean_y_error = y_error.mean()\n",
        "            mean_error = (mean_x_error + mean_y_error) / 2  # Average of x and y errors\n",
        "\n",
        "            errors.append(mean_error)\n",
        "\n",
        "            # Compute relative error in percentage\n",
        "            relative_error = (mean_error / image_size) * 100\n",
        "            relative_errors.append(relative_error)\n",
        "        else:\n",
        "            print(f\"Warning: One or more columns for landmark {i} are missing.\")\n",
        "\n",
        "    # Print average absolute errors\n",
        "    print(\"Average Absolute Errors (in pixels):\")\n",
        "    for i, error in enumerate(errors):\n",
        "        print(f\"Landmark {i+1}: {error:.4f} pixels\")\n",
        "\n",
        "    # Print relative errors\n",
        "    print(\"\\nRelative Errors (% of image size):\")\n",
        "    for i, rel_error in enumerate(relative_errors):\n",
        "        print(f\"Landmark {i+1}: {rel_error:.2f}%\")\n",
        "\n",
        "# Paths to the predictions CSV and ground truth CSV\n",
        "pred_csv = \"predictions_unet_train.csv\"\n",
        "ground_truth_csv = \"train.csv\"\n",
        "\n",
        "# Compare predictions with ground truth\n",
        "compare_predictions(pred_csv, ground_truth_csv)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-QcKWlW5nXI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}